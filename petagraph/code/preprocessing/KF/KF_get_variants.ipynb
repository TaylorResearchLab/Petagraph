{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6/22/23 - This notebook contains a workflow to extract the variants from the CHD cohort in order to count the frequency (the number) of variants throughtout the genome. We use the Chromosome location ontology (CHLO) (as of 8/1/2023 CHLO has been renamed HSCLO to make it human specific) to bin variants into 10k chunks of the genome.\n",
    "\n",
    "### RUN THIS NOTEBOOK BEFORE THE create_nodes_edges notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook was converted to jupyter notebook format from Apache zeppelin notebook format using  a command line tool ([ze2nb](https://runawayhorse001.github.io/LearningApacheSpark/ze2nb.html)). The zeppelin notebook was used to prepare data on the Kids First Variant WorkBench "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "MAF_CUTOFF = 0.01\n",
    "CHD_variants = spark.table('variants').where((F.array_contains(F.col('studies'), 'SD_PREASA7S')) & (F.col('variant_class') == 'SNV'))\\\n",
    "                                .where((F.col('gnomad_genomes_3_1_1.af') <= MAF_CUTOFF)  | (F.col(\"gnomad_genomes_3_1_1\").isNull()))   # rare freqency, or not in gnomad at all (presumably very rare)\n",
    "\n",
    "print(CHD_variants.count())\n",
    "CHD_variants.limit(50).toPandas().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "CHD_csq = spark.table('consequences').where((F.array_contains(F.col('study_ids'), 'SD_PREASA7S')) & (F.col('variant_class') == 'SNV') )\n",
    "\n",
    "# filter for high impact lower down\n",
    "CHD_csq\n",
    "print(CHD_csq.count())                               \n",
    "CHD_csq.limit(50).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "CHD_csq_pr = CHD_csq.where(F.col('biotype') == 'protein_coding') # reduce size of this df so its faster to merge\n",
    "CHD_csq_pr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "o = spark.table('occurrences').where(F.col('study_id').isin(['SD_PREASA7S']) &  (F.col('is_hi_conf_denovo') == True) & (F.col('variant_class') == 'SNV'))\n",
    "print(o.count())\n",
    "o.limit(5).toPandas().head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#cols_keep = [ 'chromosome','start','reference', 'alternate', 'end', 'hgvsg']#, 'variant_class',  'zygosity',  'transmissions',\n",
    "                                                     # 'gnomad_genomes_3_1_1', 'dbsnp_id', 'clinvar_id','clin_sig']  # 'frequencies',  '1k_genomes', 'topmed',  'name',\n",
    "\n",
    "#CHD_csq_slct = CHD_csq.where(F.col('impact') == 'HIGH').select(cols_keep)\n",
    "#CHD_csq_gene_info =  CHD_csq_pr.select( ['hgvsg','ensembl_gene_id','symbol'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "cols_keep = [ 'chromosome','start','reference', 'alternate', 'end', 'hgvsg']#, 'variant_class',  'zygosity',  'transmissions',\n",
    "                                                     # 'gnomad_genomes_3_1_1', 'dbsnp_id', 'clinvar_id','clin_sig']  # 'frequencies',  '1k_genomes', 'topmed',  'name',\n",
    "\n",
    "CHD_var_slct = CHD_variants.select(cols_keep)\n",
    "CHD_csq_slct = CHD_csq.where(F.col('impact') == 'HIGH').select(cols_keep)\n",
    "CHD_occ_slct = o.select(cols_keep) \n",
    "\n",
    "CHD_csq_gene_info =  CHD_csq_pr.select( ['hgvsg','ensembl_gene_id','symbol'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "chd_csq_occ = CHD_csq_slct.union(CHD_occ_slct).dropDuplicates()\n",
    "print(chd_csq_occ.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "j = CHD_var_slct.join(chd_csq_occ.select('hgvsg'),on='hgvsg',how='inner')\n",
    "print(j.count())\n",
    "j.show()            # 285,954\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "# test that start and hgsvg always match so we know that its a SNP?\n",
    "\n",
    "split_col = F.split(j['hgvsg'], ':g.')\n",
    "\n",
    "j_ = j.withColumn('split_hgsvg',split_col.getItem(1))\n",
    "\n",
    "j_ = j_.withColumn(\"location\",F.regexp_replace(F.col(\"split_hgsvg\"),\"(\\D)\",\"\"))\n",
    "\n",
    "#j_ = j_.select(['start','reference', 'alternate', 'name', 'end','chromosome','location','split_hgsvg'])\n",
    "\n",
    "j_.show(3)\n",
    "\n",
    "location_df = j_\n",
    "\n",
    "# taylordm/taylor-urbs-r03-kf-cardiac/files/64593f02a716c906e00175cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "chro = spark.read.csv('s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/OWLNETS_node_metadata.csv',header=True)#,sep='\\t'))\n",
    "chro = chro.toPandas()\n",
    "\n",
    "chro['chromosome'] = [i.split(':')[0].replace('chr','') for i in chro['node_id']]\n",
    "chro = chro[chro['node_id'].str.contains(':')]\n",
    "chro['start'] = [i.split(':')[-1].split('-')[0] for i in chro['node_id']]\n",
    "chro['end'] = [i.split(':')[-1].split('-')[1] for i in chro['node_id']]\n",
    "\n",
    "chro=spark.createDataFrame(chro) \n",
    "chro.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "chro = chro.withColumn(\"chromosome\", chro[\"chromosome\"].cast(IntegerType()))\\\n",
    "           .withColumn(\"start\", chro[\"start\"].cast(IntegerType()))\\\n",
    "           .withColumn(\"end\", chro[\"end\"].cast(IntegerType()))\n",
    "           \n",
    "location_df = location_df.withColumn(\"chromosome\", location_df[\"chromosome\"].cast(IntegerType()))\\\n",
    "           .withColumn(\"location\", location_df[\"location\"].cast(IntegerType()))\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#spark.read.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/delete_CHD_snp_locs_merged.parquet\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#df_merge = location_df.join(chro, (chro.chromosome == location_df.chromosome) &\\\n",
    "#                       (chro.start <= location_df.location) &\\\n",
    "#                       (chro.end >= location_df.location),'leftsemi' )\n",
    "#df_merge.count()\n",
    "\n",
    "#df_merge.write \\\n",
    "#    .parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/CHD_snp_locs_merged_2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#df = spark.read.parquet(\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/CHD_snp_locs_merged_2.parquet\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col   # why id there a null in the chromosome.result?\n",
    "#location_df.select([count(when(isnan(c), c)).alias(c) for c in location_df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "uniq_chroms = location_df.select('chromosome').distinct().toPandas().sort_values(by='chromosome').dropna()\n",
    "uniq_chroms = uniq_chroms[(uniq_chroms['chromosome']!='Y')]\n",
    "uniq_chroms = uniq_chroms[(uniq_chroms['chromosome']!='X')]\n",
    "uniq_chroms = [i[0] for i in uniq_chroms.dropna().astype(int).sort_values(by='chromosome').values]\n",
    "uniq_chroms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#location_df_CHROM =  location_df.where(F.col('chromosome') == int(22))\n",
    "#chro_CHROM =  chro.where(F.col('chromosome') == int(22))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "#chro_CHROM = chro_CHROM.sample(1/500)\n",
    "#location_df_CHROM = location_df_CHROM.sample(1/500)\n",
    "#location_df_CHROM.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "for CHROM in uniq_chroms[::-1]:\n",
    "    print(f'Merging chr{CHROM}',end='')\n",
    "    s = time.time()\n",
    "    \n",
    "    location_df_CHROM =  location_df.where(F.col('chromosome') == int(CHROM))\n",
    "    chro_CHROM =  chro.where(F.col('chromosome') == int(CHROM))\n",
    "    \n",
    "    location_df_CHROM = location_df_CHROM.drop('reference','alternate','name')\n",
    "    chro_CHROM = chro_CHROM.select(F.col('node_id'),F.col('chromosome').alias('chromosome_chro'),F.col('start').alias('start_chro'),F.col('end').alias('end_chro'))\n",
    "\n",
    "    #  (chro_CHROM.chromosome_chro == location_df_CHROM.chromosome) &\\\n",
    "    chrom_merge = location_df_CHROM.join(chro_CHROM,\n",
    "                           (chro_CHROM.start_chro <= location_df_CHROM.location) &\\\n",
    "                           (chro_CHROM.end_chro >= location_df_CHROM.location),'right' )    # do right merge. we want locations assigned to variants. we dont want locations if there arent any variants there\n",
    "    print(f'...Saving chr{CHROM}...',end='')\n",
    "    \n",
    "    chrom_merge.write.parquet(f\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/merged_snps/CHD_snp_locs_{CHROM}.parquet\")\n",
    "    e = time.time() - s\n",
    "    elapsed=e//60\n",
    "    print(f'Chr{CHROM} took {elapsed}min')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "aws s3 ls  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/merged_snps/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "merged_all = spark.read.parquet('s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/merged_snps/*.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(merged_all.count())\n",
    "merged_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "CHD_csq_gene_info.count()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "merged_all = merged_all.select(['hgvsg', 'chromosome', 'start', 'end', 'node_id'])\n",
    "merged_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "merged_all_genes = merged_all.join(CHD_csq_gene_info,on='hgvsg',how='inner')\n",
    "print(merged_all_genes.count())\n",
    "merged_all_genes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "merged_all_genes.write.parquet('s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/CHD_merged_genes_inner.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(merged_all_genes.count())\n",
    "merged_all_genes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "merged_all_genes.write.parquet(f\"s3a://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/CHD_merged_genes.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "print(CHD_csq_gene_info.count())\n",
    "CHD_csq_gene_info.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "#aws s3 cp   ~/cavatica/projects/taylordm/taylor-urbs-r03-kf-cardiac/OWLNETS_node_metadata.csv  s3://kf-strides-variant-parquet-prd/notebooks/5175e6e3-c3d7-4c19-b51f-6f1ea4dd3700/DataDistillery/OWLNETS_node_metadata.csv\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "cd ~/.sevenbridges/\n",
    "if [[ -e 'credentials' ]]; then mv credentials credentials.bak; fi\n",
    "cat << EOF > credentials\n",
    "[default]\n",
    "api_endpoint = https://cavatica-api.sbgenomics.com/v2\n",
    "auth_token   = c05edbf9ffe041479f063666e23f675f\n",
    "EOF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "cd ~\n",
    "if ! [[ -x 'sbfs' ]]; then curl https://igor.sbgenomics.com/downloads/sbfs/linux-amd64/sbfs -O; chmod 755 sbfs; fi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "format": "text/plain"
   },
   "source": [
    "%sh\n",
    "cd ~\n",
    "! [[ -x 'cavatica' ]] && mkdir ~/cavatica\n",
    "[[ \"$(ls -A ~/cavatica)\" ]] || ~/sbfs mount ~/cavatica\n",
    "echo 'Wait until mounting is done ...'\n",
    "while [[ -e ~/cavatica/mount_status ]]; do sleep 1; done\n",
    "ls -l ~/cavatica/projects/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
