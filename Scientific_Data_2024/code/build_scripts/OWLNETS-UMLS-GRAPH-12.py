#!/usr/bin/env python
# coding: utf-8

# OCTOBER 2022
# JAS
# THIS VERSION OF THE SCRIPT (OWLNETS-UMLS-GRAPH-12.py) IS THE SCRIPT OF RECORD.
# ENHANCEMENTS OR BUG FIXES SHOULD BE MADE DIRECTLY TO THIS SCRIPT.

# In other words, the method of updating a Jupyter notebook to a new version and then
# running the transform script to convert the notebook to a pure Python script has been deprecated.
# "Version 12" of the script is the source of coding truth.
#

# -----------------------------------------------------

# # OWLNETS-UMLS-GRAPH

# ## Appends to a set of existing ontology CSV files content from a set of assertions. The assertions are in
# one of two formats:
# 1. OWLNETS files generated by running the PhenKnowLator over an OWL file
# 2. "UBKG Edges/Nodes".
# The set of assertions can be from an ontology or another data source.

# This script is called from build_csv.py with the following arguments:
# 1. The path to the OWLNETS directory (default: owlnets_output) where the assertion data files are located
# 2. path to the directory that contains the ontology CSV files
# 3. SAB for the set of assertions/ontology that is being ingested
# sys.argv is used instead of argparse because this script is designed to be called as a subprocess.

# -----------------------------------------------------
# ### SETUP

# In[1]:

import sys
import pandas as pd
import numpy as np
import base64
import json
import os
from tqdm import tqdm
# from tqdm.auto import tqdm # for notebooks

# Create new `pandas` methods which use `tqdm` progress
# (can use tqdm_gui, optional kwargs, etc.)
tqdm.pandas()

# JAS FEB 2023
# The codeReplacements function was originally part of this script. Because the function is now
# needed by both this script and the skowlnets.py script, the function has been moved to the module
# generation_framework/ubkg_utilities/ubkg_parsetools.py.
# The following allows for an absolute import from an adjacent script directory--i.e., up and over instead of down.
# Find the absolute path. (This assumes that this script is being called from build_csv.py.)
fpath = os.path.dirname(os.getcwd())
fpath = os.path.join(fpath, 'generation_framework/ubkg_utilities')
sys.path.append(fpath)
import ubkg_parsetools as uparse
import ubkg_extract as uextract
import ubkg_logging as ulog
import ubkg_reporting as ureport
import ubkg_clean_csv as uclean


def owlnets_path(file: str) -> str:
    # Appends the OWLNETS path to the file argument.
    return os.path.join(sys.argv[1], file)


def csv_path(file: str) -> str:
    # Appends the CSV path to the file argument.
    return os.path.join(sys.argv[2], file)


def identify_source_file(file_names: list) -> str:

    # Checks for the existence of source files (edges or nodes).

    # Source files can be named in various ways--e.g., OWLNETS_node_metadata.txt, nodes.tsv, nodes.txt

    for f in file_names:
        if os.path.exists(owlnets_path(f)):
            return f

    # Error case: no file found with name in argument list.
    lfile = ','.join(str(f) for f in file_names)
    raise FileNotFoundError('No file found with name in list: ' + lfile)


def getROrelationshiptriples() -> pd.DataFrame:

    # -------------------------------------------------------------
    # IDENTIFY RELATIONSHIPS AND INVERSE RELATIONSHIPS

    # Obtain descriptions of relationships and their inverses from the Relations Ontology JSON.

    # The Relations Ontology (RO) is an ontology of relationships, in which the nodes are
    # relationship properties and the edges (predicates) are relationships *between*
    # relationship properties.
    # For example,
    # relationship property RO_0002292 (node) inverseOf (edge) relationship property RO_0002206 (node)
    # or
    # "expresses" inverseOf "expressed in"

    ulog.print_and_logger_info('-- Obtaining relationship reference information from Relations Ontology...')
    # Fetch the RO JSON.
    dfro = pd.read_json("https://raw.githubusercontent.com/oborel/obo-relations/master/ro.json")

    # Information on relationship properties (i.e., relationship property nodes) is in the node array.
    dfnodes = pd.DataFrame(dfro.graphs[0]['nodes'])
    # Information on edges (i.e., relationships between relationship properties) is in the edges array.
    dfedges = pd.DataFrame(dfro.graphs[0]['edges'])

    # Information on the relationships between relationship properties *should be* in the edges array.
    # Example of edge element:
    # {
    #      "sub" : "http://purl.obolibrary.org/obo/RO_0002101",
    #      "pred" : "inverseOf",
    #      "obj" : "http://purl.obolibrary.org/obo/RO_0002132"
    #    }
    #
    # The ontology graph requires that every relationship have an inverse.
    # Not all relationships in RO are defined with inverses; for these relationships, the script will create
    # "pseudo-inverse" relationships--e.g., if the only information available is the label "eats", then
    # the pseudo-inverse will be "inverse_eats" (instead of, say, "eaten_by").

    # Cases that require pseudo-inverses include:
    # 1. A property is incompletely specified in terms of both sides of an inverse relationship--e.g.,
    #    RO_0002206 is listed as the inverse of RO_0002292, but RO_0002292 is not listed as the
    #    corresponding inverse of RO_0002206. For these properties, the available relationship
    #    will be inverted when joining relationship information to the edgelist.
    #    (This is really a case in which both directions of the inverse relationship should have been
    #    defined in the edges node, but were not.)
    # 2. A property does not have inverse relationships defined in RO.
    #    The relationship will be added to the list with a null inverse. The script will later create a
    #    pseudo-inverse by appending "inverse_" to the relationship label.

    # ---------------------------------

    # Obtain triple information for relationship properties--i.e.,
    # 1. IRIs for "subject" nodes and "object" nodes (relationship properties)
    # 2. relationship predicates (relationships between relationship properties)

    # Get subject node, edge
    # MAY 2023 replace inner join with left
    dfrt = dfnodes.merge(dfedges, how='left', left_on='id', right_on='sub')
    # Get object node
    dfrt = dfrt.merge(dfnodes, how='left', left_on='obj', right_on='id')

    # May 2023
    # Set a default predicate to capture nodes without predicates.
    dfrt = dfrt.fillna(value={'pred': 'no predicate'})

    # ---------------------------------
    # Identify relationship properties that do not have inverses.
    # 1. Group relationship properties by predicate, using count.
    #    ('pred' here describes the relationship between relationship properties.)
    dfpred = dfrt.groupby(['id_x', 'pred']).count().reset_index()

    # 2. Identify the relationships for which the set of predicates does not include "inverseOf".
    listinv = dfpred[dfpred['pred'] == 'inverseOf']['id_x'].to_list()
    listnoinv = dfpred[~dfpred['id_x'].isin(listinv)]['id_x'].to_list()
    dfnoinv = dfrt.copy()
    dfnoinv = dfnoinv[dfnoinv['id_x'].isin(listnoinv)]

    # 3. Rename column names to match the relationtriples frame. (Column names are described
    #    farther down.)
    dfnoinv = dfnoinv[['id_x', 'lbl_x', 'id_y', 'lbl_y']].rename(
        columns={'id_x': 'IRI', 'lbl_x': 'relation_label_RO', 'id_y': 'inverse_IRI', 'lbl_y': 'inverse_RO'})
    # The inverses are undefined.
    dfnoinv['inverse_IRI'] = np.nan
    dfnoinv['inverse_RO'] = np.nan

    # ---------------------------------
    # Look for members of incomplete inverse pairs--i.e., relationship properties that are
    # the *object* of an inverseOf edge, but not the corresponding *subject* of an inverseOf edge.
    #
    # 1. Filter edges to inverseOf.
    dfedgeinv = dfedges[dfedges['pred'] == 'inverseOf']

    # 2. Find all relation properties that are objects of inverseOf edges.
    dfnoinv = dfnoinv.merge(dfedgeinv, how='left', left_on='IRI', right_on='obj')

    # 3. Get the label for the relation properties that are subjects of inverseOf edges.
    dfnoinv = dfnoinv.merge(dfnodes, how='left', left_on='sub', right_on='id')
    dfnoinv['inverse_IRI'] = np.where(dfnoinv['lbl'].isnull(), dfnoinv['inverse_IRI'], dfnoinv['id'])
    dfnoinv['inverse_RO'] = np.where(dfnoinv['lbl'].isnull(), dfnoinv['inverse_RO'], dfnoinv['lbl'])
    dfnoinv = dfnoinv[['IRI', 'relation_label_RO', 'inverse_IRI', 'inverse_RO']]

    # ---------------------------------
    # Filter the base triples frame to just those relationship properties that have inverses.
    # This step eliminates relationship properties related by relationships such as "subPropertyOf".
    dfrt = dfrt[dfrt['pred'] == 'inverseOf']

    # Rename column names.
    # Column names will be:
    # IRI - the IRI for the relationship property
    # relation_label_RO - the label for the relationship property
    # inverse_RO - the label of the inverse relationship property
    # inverse_IRI - IRI for the inverse relationship (This will be dropped.)
    dfrt = dfrt[['id_x', 'lbl_x', 'id_y', 'lbl_y']].rename(
        columns={'id_x': 'IRI', 'lbl_x': 'relation_label_RO', 'id_y': 'inverse_IRI', 'lbl_y': 'inverse_RO'})

    # Add triples for problematic relationship properties--i.e., without inverses or from incomplete pairs.
    dfrt = pd.concat([dfrt, dfnoinv], ignore_index=True).drop_duplicates(subset=['IRI'])

    # Convert stings for labels for relationships to expected delimiting.
    dfrt['relation_label_RO'] = \
        dfrt['relation_label_RO'].str.replace(' ', '_').str.split('/').str[-1]
    dfrt['inverse_RO'] = dfrt['inverse_RO'].str.replace(' ', '_').str.split('/').str[-1]

    dfrt = dfrt.drop(columns='inverse_IRI')

    # March 2024 Cast IRI to lowercase.
    dfrt['IRI'] = dfrt['IRI'].str.lower()

    return dfrt


# -----------------------------------------------------
# START OF SCRIPT
# Assignment of SAB for CUI-CUI relationships (edgelist) - typically use file name before .owl in CAPS
OWL_SAB = sys.argv[3].upper()

# Threshold number of rows for which to show TQDM progress bars when writing to output.
TQDM_THRESHOLD = 100000

# Start QC report.
qcpath = owlnets_path('ubkg_qc.txt')
ubkg_report = ureport.UbkgReport(path=qcpath, sab=OWL_SAB)

pd.set_option('display.max_colwidth', None)

# -------------------------------------------------------------
# ### Ingest OWLNETS output files, remove NaN and duplicate (keys) if they were to exist

# In[2]:

ulog.print_and_logger_info('READING DATA FILES (edges, nodes, relations)...')

# JAS 6 JAN 2023
# The node file will be in one of two formats:
# 1. OWLNETS
# 2. UBKG edge/nodes

nodefilelist = ['OWLNETS_node_metadata.txt', 'nodes.txt', 'nodes.tsv', 'OWLNETS_node_metadata.tsv']
nodepath = identify_source_file(nodefilelist)


# JAS 6 JAN 2023 add optional columns (value, lowerbound, upperbound, unit) for UBKG edges/nodes files.
# JAS 6 JAN 2023 skip bad rows. (There is at least one in line 6814 of the node_metadata file generated from EFO.)
ulog.print_and_logger_info('-- Reading nodes file...')
# node_metadata = pd.read_csv(owlnets_path(nodepath), sep='\t', on_bad_lines='skip')
node_metadata = uextract.read_csv_with_progress_bar(path=owlnets_path(nodepath), on_bad_lines='skip', sep='\t')

if 'value' not in node_metadata.columns:
    # This set of assertions does not have optional property/relationship columns.
    node_metadata['value'] = np.nan
    node_metadata['lowerbound'] = np.nan
    node_metadata['upperbound'] = np.nan
    node_metadata['unit'] = np.nan

# June 2023
# The following fields are not technically required in the node file. For example, if a set of assertions
# contains only relationships between nodes from other ontologies, or if the owner of nodes does not provide
# additional information, the nodes file might contain only a node_id field. Instead of insisting that
# the nodes file have the optional field headers, simply add them in.
# node_label
# node_definition
# node_synonyms
# node_dbxrefs
node_optional_fields = ['node_label', 'node_definition', 'node_synonyms', 'node_dbxrefs']
for node in node_optional_fields:
    if node not in node_metadata.columns:
        node_metadata[node] = np.nan

# JAS Feb 2023 ignore 'node_namespace'. This is an artifact of the PheKnowLator output.
# node_metadata = node_metadata[['node_id', 'node_namespace', 'node_label', 'node_definition', 'node_synonyms',
                               # 'node_dbxrefs', 'value', 'lowerbound', 'upperbound', 'unit']]


node_metadata = node_metadata[['node_id', 'node_label', 'node_definition', 'node_synonyms',
                               'node_dbxrefs', 'value', 'lowerbound', 'upperbound', 'unit']]

ulog.print_and_logger_info('---- Dropping duplicate nodes...')
# Replace 'None' with nan
# August 2023 - Set blank values of value, etc. to blank instead of NAN for consistent row length.
# node_metadata = node_metadata.replace({'None': np.nan})
node_metadata = node_metadata.replace({'None': ''})

# Drop nodes for which node_id is nan.
# Drop duplicate nodes.
# The subset is necessary because the node file may contain optional columns with cells with no data.
node_metadata = node_metadata.dropna(subset=['node_id']).drop_duplicates(subset='node_id').reset_index(drop=True)

# June 2023 only process nodes for terms if labels are available.
node_metadata_has_labels = len(node_metadata['node_label'].value_counts()) > 0

# June 2023 Use case: GlyGen PROTEOFORM, in which the label column had very few values at the bottom,
# all of which are numeric.
# Pandas assigns a data type of float to the entire column, which causes issues for attempts to merge
# with files like SUIs.csv, for which all columns are assigned to type object.
# Explicitly cast the 'node_label' column as object.
node_metadata['node_label'] = node_metadata['node_label'].astype(object)

# -------------------------------------------------------
# OPTIONAL RELATIONS INFORMATION

# In[3]:

# JAS 8 Nov 2022
# The OWLNETS_relations.txt file is no longer required; however, if it is available, it will contain information for
# relation properties that are defined outside the Relations Ontology.
#
# Obtain relationship information as follows:
# 1. If a relations file is available (e.g., as output from PheKnowLator), obtain
# relationship label information from it.
# 2. If no relations file is available, obtain relationship information from the
# OWLNETS_edgelist.txt.
#
# An edge in edgelist is one of the following types:
# 1. an IRI that corresponds to the IRI of a relationship property from the Relations Ontology (RO)
# 2. a subClassOf relationship, that will be translated to isa
# 3. a text string that matches a relationship label in RO--i.e., a weaker link to RO than by IRI.
# 4. a text string that does not match a relationship label in RO.
#    This can either be a relationship that is defined with an IRI not in RO or a
#    custom string.
# Possible enhancement: identify relationship properties outside of RO. This would entail finding
# equivalents of ro.json, or perhaps some form of API call.
ulog.print_and_logger_info('-- Reading optional relations file...')
relations_file_exists = os.path.exists(owlnets_path('OWLNETS_relations.txt'))

if relations_file_exists:
    ulog.print_and_logger_info('---- Relations metadata found; obtaining relations data from relations metadata file.')
    relations = pd.read_csv(owlnets_path("OWLNETS_relations.txt"), sep='\t')
    relations = relations.replace({'None': np.nan})
    relations = relations.dropna(subset=['relation_id']).drop_duplicates(subset='relation_id').reset_index(drop=True)
    # handle relations with no label by inserting part after # - may warrant more robust solution or a hard stop
    relations.loc[relations['relation_label'].isnull(), 'relation_label'] = relations['relation_id'].str.split('#').str[
        -1]
    # Format edges.
    relations['relation_id'] = uparse.relationReplacements(relations['relation_id'])
    relations['relation_label'] = uparse.relationReplacements(relations['relation_label'])

else:
    ulog.print_and_logger_info('---- No relations metadata found; obtaining relations data from edgelist file.')


# -------------------------------------------------------
# EDGE LIST INFORMATION

# In[4]:

# JAS 6 JAN 2023
# The edgelist file will be in one of two formats:
# 1. OWLNETS
# 2. UBKG edge/nodes

ulog.print_and_logger_info('-- Reading edge file...')
edgefilelist = ['OWLNETS_edgelist.txt', 'edges.txt', 'edges.tsv', 'OWLNETS_edgelist.tsv']
edgepath = identify_source_file(edgefilelist)

ulog.print_and_logger_info('---- Dropping duplicates, empty rows, and self-referential edges...')
# JAS 6 JAN 2023 - add evidence_class; limit columns.
edgelist = uextract.read_csv_with_progress_bar(path=owlnets_path(edgepath), on_bad_lines='skip', sep='\t')

# MARCH 2024 - Casting predicate to lowercase.
edgelist['predicate'] = uparse.relationReplacements(edgelist['predicate'])

# edgelist = pd.read_csv(owlnets_path(edgepath), sep='\t')
if 'evidence_class' not in edgelist.columns:
    # August 2023 - set to blank instead of NAN for consistent row content.
    # edgelist['evidence_class'] = np.nan
    edgelist['evidence_class'] = ''
edgelist = edgelist[['subject', 'predicate', 'object', 'evidence_class']]

# JAS 6 JAN 2023 - Add subset to dropna because evidence_class is optional.
subset = ['subject', 'predicate', 'object']
edgelist = edgelist.dropna(subset=subset).drop_duplicates(subset=subset).reset_index(drop=True)
edgelist = edgelist.replace({'None': np.nan})

# #### Delete self-referential edges in edgelist - CUI self-reference also avoided (later) by unique CUIs for node_ids
# In[5]:

edgelist = edgelist[edgelist['subject'] != edgelist['object']].reset_index(drop=True)

# ---------------------------------
# CHECK FOR LARGE NUMBERS OF NODES OR EDGES

# JAS 20 OCT 2022
# The OWLNETS files for PR are large enough to cause the script to run out of memory in the CUI assignments.
# So, a hack and a logging solution:
# 1. For PR, only select those nodes for the ORGANISM specified in the optional argument
#    (e.g., human, mouse).
#    This will require text searching, and so is a hack that depends on the node description field
#    containing a key word.
# 2. Show counts of rows and warn for large files.

edgecount = len(edgelist.index)
nodecount = len(node_metadata.index)
ulog.print_and_logger_info('-- Checking file statistics...')
ulog.print_and_logger_info(f'---- Number of edges in edgelist.txt: {edgecount}')
ulog.print_and_logger_info(f'---- Number of nodes in node_metadata.txt: {nodecount}')

if edgecount > 500000 or nodecount > 500000:
    ulog.print_and_logger_info('***** WARNING: Large data files may cause the script to terminate from memory issues.')


# ---------------------------------------------
# ESTABLISH EDGES AND INVERSE EDGES.

# ### Join relation_label in edgelist, convert subClassOf to isa and space to _, CodeID formatting

# In[7]:
ulog.print_and_logger_info('IDENTIFYING EDGES AND INVERSE EDGES...')

# JAS 8 Nov 2022
# The OWLNETS_relations.txt file is no longer required; however, if it is available, it will contain information for
# relation properties that are defined outside the Relations Ontology.

if relations_file_exists:
    # Obtain the relation label from the relations file.
    # This can correspond to:
    # 1. The label for a relationship property in RO.
    # 2. The label for a relationship not defined in RO, including relationships defined in
    #    other ontologies.
    # JAS 6 JAN 2023 Add evidence_class
    edgelist = edgelist.merge(relations, how='left', left_on='predicate', right_on='relation_id')
    edgelist = edgelist[['subject', 'predicate', 'object', 'relation_label', 'evidence_class']].rename(
        columns={"relation_label": "relation_label_from_file"})
else:
    edgelist['relation_label_from_file'] = np.NaN

# del relations

# JAS 8 November 2022 - now handled downstream
# edgelist.loc[(edgelist.relation_label == 'subClassOf'), 'relation_label'] = 'isa'
# edgelist['relation_label'] = edgelist['relation_label'].str.replace(' ', '_')

# FORMAT SUBJECT AND OBJECT NODE Code IDs.

# JAS string replacements moved to codeReplacements function.
# edgelist['subject'] = \
# edgelist['subject'].str.replace(':', ' ').str.replace('#', ' ').str.replace('_', ' ').str.split('/').str[-1]
ulog.print_and_logger_info('-- Formatting node IDs...')
edgelist['subject'] = uparse.codeReplacements(edgelist['subject'], OWL_SAB)

# JAS 15 NOV 2022
# Deprecate all prior code that handled object nodes, including from October 2022, in favor of the
# improved codeReplacements function.
edgelist['object'] = uparse.codeReplacements(edgelist['object'], OWL_SAB)

# MARCH 2024 - moved to prior point in script.
# MAY 2023
# Format edges.
# edgelist['predicate'] = uparse.relationReplacements(edgelist['predicate'])

# Obtain relationship information from the Relations Ontology.
dfrelationtriples = getROrelationshiptriples()

# ---------------------------------------------------------
# JAS 8 NOV 2022
# JOIN RELATIONS WITH EDGES
# Rewrite of join logic to account for:
# 1. Optional use of OWLNETS_relations.txt file.
# 2. Identification of all relations in Relations Ontology, not just inverses.

# Perform a series of joins and rename resulting columns to keep track of the source of
# relationship labels and inverse relationship labels.

ulog.print_and_logger_info('-- Translating predicates in edges file to relationships from Relations Ontology...')

# Check for relationships in RO, considering the edgelist predicate as a *full IRI*.
edgelist = edgelist.merge(dfrelationtriples, how='left', left_on='predicate',
                          right_on='IRI').drop_duplicates().reset_index(drop=True)

# JAS 6 JAN 2023 add optional evidence_class
edgelist = edgelist[
    ['subject', 'predicate', 'object', 'evidence_class', 'relation_label_from_file', 'relation_label_RO',
     'inverse_RO']].rename(
    columns={'relation_label_RO': 'relation_label_RO_fromIRIjoin', 'inverse_RO': 'inverse_RO_fromIRIjoin'})

# Check for relationships in RO by label, considering the edgelist predicate as *a label*--e.g.,
# as a simplified version of an IRI.
# First, format the predicate string to match potential relationship strings from RO.
# Parsing note: relationship IRIs often include the '#' character as a terminal delimiter, and
# be in format url...#relation--e.g., ccf.owl#ct_is_a.
edgelist['predicate'] = edgelist['predicate'].str.replace(' ', '_').str.replace('#', '/').str.split('/').str[-1]
edgelist = edgelist.merge(dfrelationtriples, how='left', left_on='predicate',
                          right_on='relation_label_RO').drop_duplicates().reset_index(drop=True)

# JAS 6 JAN 2023 add optional evidence_class
edgelist = edgelist[
    ['subject', 'predicate', 'object', 'evidence_class', 'relation_label_from_file', 'relation_label_RO_fromIRIjoin',
     'inverse_RO_fromIRIjoin', 'relation_label_RO', 'inverse_RO']].rename(
    columns={'relation_label_RO': 'relation_label_RO_frompredicatejoinlabel',
             'inverse_RO': 'inverse_RO_frompredicatejoinlabel'})

# Check for relationships in RO by label, considering the relationship label from the
# OWLNETS_relations.txt file that corresponds to the predicate (if available).
if relations_file_exists:
    edgelist['relation_label_from_file'] = \
        edgelist['relation_label_from_file'].str.replace(' ', '_').str.replace('#', '/').str.split('/').str[-1]
    edgelist = edgelist.merge(dfrelationtriples, how='left', left_on='relation_label_from_file',
                              right_on='relation_label_RO').drop_duplicates().reset_index(drop=True)
    # JAS 6 JAN 2023 add optional evidence_class
    edgelist = edgelist[['subject', 'predicate', 'object', 'evidence_class', 'relation_label_from_file',
                         'relation_label_RO_fromIRIjoin',
                         'inverse_RO_fromIRIjoin', 'relation_label_RO_frompredicatejoinlabel',
                         'inverse_RO_frompredicatejoinlabel', 'relation_label_RO', 'inverse_RO']].rename(
        columns={'relation_label_RO': 'relation_label_RO_fromfilelabeljoinlabel',
                 'inverse_RO': 'inverse_RO_fromfilelabeljoinlabel'})
# JAS APR 2023 - Add null columns if no relations file.
else:
    edgelist['relation_label_RO_fromfilelabeljoinlabel'] = np.nan
    edgelist['inverse_RO_fromfilelabeljoinlabel'] = np.nan

# JAS APR 2023 - Check for relationships in RO, considering the edgelist predicate as an abbreviated IRI
# in format RO:code. (Use case: GTEX)
# First, try to reformat the predicate string as a full IRI.
edgelist['predicate_expanded'] = 'http://purl.obolibrary.org/obo/' + edgelist['predicate'].str.replace(':', '_')
edgelist = edgelist.merge(dfrelationtriples, how='left', left_on='predicate_expanded',
                          right_on='IRI').drop_duplicates().reset_index(drop=True)

edgelist = edgelist[['subject', 'predicate', 'object', 'evidence_class', 'relation_label_from_file',
                         'relation_label_RO_fromIRIjoin', 'inverse_RO_fromIRIjoin',
                         'relation_label_RO_frompredicatejoinlabel', 'inverse_RO_frompredicatejoinlabel',
                         'relation_label_RO_fromfilelabeljoinlabel', 'inverse_RO_fromfilelabeljoinlabel',
                         'relation_label_RO', 'inverse_RO']].rename(
        columns={'relation_label_RO': 'relation_label_RO_fromexpandedpredicate',
                 'inverse_RO': 'inverse_RO_fromexpandedpredicate'})

# We now have labels for relations and inverses for the following scenarios:
# 1. relation_label_RO_fromIRIjoin/inverse_fromIRIjoin - predicate was a full IRI that was in RO
# 2. relation_label_RO_fromlabeljoin/inverse_fromlabeljoin - predicate corresponds to the label
#    of a relation property in RO
# 3. relation_label_from_file/(null inverse) - relation label from the OWLNETS_relations.txt.
#    The label is either the label of a relationship with IRI not in RO or a custom relationship label.
# 4. predicate/(null inverse) - a custom relationship label
# JAS APR 2023
# 5. relation_label_RO_fromexpandedpredicate/inverse_RO_fromexpandedpredicate -
#    The predicate was formatted as RO:code instead of as a full IRI.

# Order of precedence for relationship/inverse relationship data:
# 1. label from the edgelist predicate joined to RO by IRI
# JAS APR 2023
# 2. label from the edgelist predicate formatted as RO:code, joined to RO by IRI
# JAS APR 2023
# 3. label from the edgelist predicate joined to RO by label
# 4. label from OWLNETS_relations.txt, joined against RO
# 5. label from OWLNETS_relations.txt, not joined against RO
# 6. predicate from edgelist
# 7. 'subClassOf' predicates converted to 'isa'
# JAS SEPT 2023 - #8 moved to relationreplacements in ubkg_parsetools.py
# 8. JAS 13 JAN 2023 - 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type' converted to 'isa'


edgelist['relation_label'] = edgelist['relation_label_RO_fromIRIjoin']

# JAS APR 2023
edgelist['relation_label'] = np.where(edgelist['relation_label'].isnull(),
                                      edgelist['relation_label_RO_fromexpandedpredicate'], edgelist['relation_label'])

edgelist['relation_label'] = np.where(edgelist['relation_label'].isnull(),
                                      edgelist['relation_label_RO_frompredicatejoinlabel'], edgelist['relation_label'])
if relations_file_exists:
    edgelist['relation_label'] = np.where(edgelist['relation_label'].isnull(),
                                          edgelist['relation_label_RO_fromfilelabeljoinlabel'],
                                          edgelist['relation_label'])
    edgelist['relation_label'] = np.where(edgelist['relation_label'].isnull(), edgelist['relation_label_from_file'],
                                          edgelist['relation_label'])
edgelist['relation_label'] = np.where(edgelist['relation_label'].isnull(), edgelist['predicate'],
                                      edgelist['relation_label'])
edgelist['relation_label'] = np.where(edgelist['predicate'].str.contains('subClassOf'), 'isa',
                                      edgelist['relation_label'])

# JAS MARCH 2024 - relationships are cast to lowercase.
edgelist['relation_label'] = np.where(edgelist['predicate'].str.contains('subclassof'), 'isa',
                                      edgelist['relation_label'])

# JAS SEPT 2023 - Moved to relationshipreplacements in ubkg_parsetools.py
# JAS 13 JAN 2023 - 'http://www.w3.org/1999/02/22-rdf-syntax-ns#type' converted to 'isa'
#edgelist['relation_label'] = np.where(edgelist['predicate'].str.contains('http://www.w3.org/1999/02/22-rdf-syntax-ns#type'),
                                      #'isa', edgelist['relation_label'])

# The algorithm for inverses is simpler: if one was derived from RO, use it; else leave empty, and
# the script will create a pseudo-inverse.

edgelist['inverse'] = edgelist['inverse_RO_fromIRIjoin']

edgelist['inverse'] = np.where(edgelist['inverse'].isnull(), edgelist['inverse_RO_fromexpandedpredicate'],
                               edgelist['inverse'])

edgelist['inverse'] = np.where(edgelist['inverse'].isnull(), edgelist['inverse_RO_frompredicatejoinlabel'],
                               edgelist['inverse'])
edgelist['inverse'] = np.where(edgelist['inverse'].isnull(), edgelist['inverse_RO_fromexpandedpredicate'],
                               edgelist['inverse'])
if relations_file_exists:
    edgelist['inverse'] = np.where(edgelist['inverse'].isnull(), edgelist['inverse_RO_fromfilelabeljoinlabel'],
                                   edgelist['inverse'])

# In[9]:

# JAS 8 NOV 2022 -
# Original code that assumed that joins were only to inverse relations.
# edgelist = edgelist.merge(df, how='left', left_on='relation_label', right_on='lbl').drop_duplicates().reset_index(
# drop=True)
# edgelist.drop(columns=['lbl'], inplace=True)
# del df

# #### Add unknown inverse_ edges (i.e., pseudo-inverses)

# In[10]:

edgelist.loc[edgelist['inverse'].isnull(), 'inverse'] = 'inverse_' + edgelist['relation_label']

# ---------------------------------------------------------
# PREPARE NODES

# ### Clean up node_metadata
# Objectives:
# 1. Standardize the format of the code for the node.
# 2. If the node has synonyms, prepare the synonym list to be a source of synonym Term nodes.
# 3. Associate the node with a CUI in order of preference:
#    a. the UMLS CUI of a cross-reference (from the original UMLS CSVs)
#    b. the non-UMLS CUI of a cross-reference (previously ingested in the ontology CSVs from a non-UMLS ontology)
#    c. the existing UMLS CUI of the node
#    d. the new non-UMLS CUI of the node

# In[11]:

ulog.print_and_logger_info('Translating node metadata...')
# JUNE 2023
# Drop duplicate nodes.
node_metadata = node_metadata.drop_duplicates(subset=['node_id']).reset_index(drop=True)

# JAS 15 November string replacements moved to codeReplacements function.
# node_metadata['node_id'] = \
# node_metadata['node_id'].str.replace(':', ' ').str.replace('#', ' ').str.replace('_', ' ').str.split('/').str[-1]

# Standardize format for node codes.
ulog.print_and_logger_info('-- Formatting node ids...')
node_metadata['node_id'] = uparse.codeReplacements(node_metadata['node_id'], OWL_SAB)

# --------------------------------------------------
# QC REPORTING, workflow point 1 - after nodes and edges have been harmonized.
ulog.print_and_logger_info('QC report: writing statistics on nodes and edges after reading and parsing.')
ubkg_report.edgelist = edgelist
ubkg_report.node_metadata = node_metadata
ubkg_report.report_file_statistics()
ubkg_report.report_edge_node_statistics()
# --------------------------------------------------

# NODE SYNONYMS
# original comment: synonyms .loc of notna to control for owl with no syns

# Synonyms for a node are in the node_synonyms column as a pipe-delimited string--e.g., "synonym 1|synonym 2".
# Split the delimited string into a list. Each list element will become a separate Term node with a relationship
# type SY with the node code.
ulog.print_and_logger_info('-- Preparing synonyms...')
node_metadata.loc[node_metadata['node_synonyms'].notna(), 'node_synonyms'] = \
    node_metadata[node_metadata['node_synonyms'].notna()]['node_synonyms'].astype('str').str.split('|')

ulog.print_and_logger_info('-- Preparing cross-references: splitting and formatting...')
# CROSS-REFERENCES
# The dbxrefs column is, in general, a string in format
# SAB<SAB-CODE delimiter>CODE<CODE-CODE delimiter>...
# e.g., SAB1:Code1|SAB2:Code2
# The accepted CODE-CODE delimiter is a pipe; the SAB-CODE delimiter can be either a colon or a space.

# original comment: dbxref .loc of notna to control for owl with no dbxref

# 5 OCT 2022 - JAS: notice the conversion to uppercase. This requires a corresponding
#                   conversion when merging with CUI-CODEs data.

# For every row with a non-na string in node_dbxrefs, replace colons with spaces--
# i.e., standardize the SAB-CODE delimiter.
# e.g., node1 / SAB1:Code1|SAB2:Code2 => node 1 / SAB1 Code1|SAB2 Code2

# JULY 2023 - Deprecated replacement of colons with spaces. *COLON* IS THE SAB-CODE DELIMITER.
node_metadata.loc[node_metadata['node_dbxrefs'].notna(), 'node_dbxrefs'] = \
    node_metadata[node_metadata['node_dbxrefs'].notna()]['node_dbxrefs'].astype('str').str.upper()#.str.replace(':', ' ')

# JULY 2023 - In-line documentation modified to assumption of colon as SAB:code delimiter.

# Convert the dbxref strings to a list, splitting on the CODE-CODE delimiter.
# e.g., node1 / SAB1:Code1|SAB2:Code2 => node1/ [SAB1:Code1, SAB2:Code2]
node_metadata['node_dbxrefs'] = node_metadata['node_dbxrefs'].str.split('|')

# Explode node_metadata to a DataFrame in which each row from node_metadata maps a node to a single cross-reference
# from the list.
# e.g., nodeID / node_dbxrefs
#       node1/ [SAB1:Code1, SAB2:Code2] =>
#
#       nodeID / node_dbxrefs
#       node1 / SAB1:Code1
#       node1 / SAB2:Code2

explode_dbxrefs = node_metadata.explode('node_dbxrefs')[['node_id', 'node_dbxrefs']].dropna().astype(
    str).drop_duplicates().reset_index(drop=True)

# Standardize the codes for the cross-references.
explode_dbxrefs['node_dbxrefs'] = uparse.codeReplacements(explode_dbxrefs['node_dbxrefs'], OWL_SAB)

# Create SAB and CODE columns from the node_id.
# JAS 12 JAN 2023 - force uppercase for SAB
# JULY 2023 - Colon is the SAB:CODE delimiter.
node_metadata['SAB'] = node_metadata['node_id'].str.split(':').str[0].str.upper()
node_metadata['CODE'] = node_metadata['node_id'].str.split(':').str[-1]

# del node_metadata['node_namespace']
# del explode_dbxrefs - not deleted here because we need it later

# --------------------------------------------------------------
# ASSOCIATE NODES WITH CROSS-REFERENCES THAT ARE UMLS CUIs
# --i.e., if the cross-references were already in the UMLS CSVs.

# ### Get the UMLS CUIs for each node_id as nodeCUIs

# In general, a cross-reference can be either to a *code* in another ontology (e.g., CL:9999999) or to
# a UMLS CUI (e.g., UMLS:C999999). If the cross-reference is to a UMLS CUI, obtain the CUI.

# In[12]:

ulog.print_and_logger_info('-- Identifying cross-references that are UMLS CUIs...')

# Separate code/CUI from SAB for each cross-reference.
# JULY 2023 - Colon is SAB:Code delimiter
explode_dbxrefs['nodeXrefCodes'] = explode_dbxrefs['node_dbxrefs'].str.split(':').str[-1]

# --------------------------------------------------
# QC REPORTING, workflow point 2 - after dbxrefs are processed
ulog.print_and_logger_info('QC report: writing statistics on nodes after dbxref expansion and parsing.')
ubkg_report.explode_dbxrefs = explode_dbxrefs
ubkg_report.report_dbxref_statistics()
# --------------------------------------------------


# JAS JAN 2023 - added group_keys=False to silence the FutureWarning from Pandas.
# Identify cross-references that are UMLS CUIs.
#explode_dbxrefs_UMLS = \
    #explode_dbxrefs[explode_dbxrefs['node_dbxrefs'].str.contains('UMLS C') == True].groupby('node_id', sort=False, group_keys=False)[
        #'nodeXrefCodes'].apply(list).reset_index(name='nodeCUIs')
# JUlY 2023 - Colon is the delimiter between SAB:CODE (in this case, SAB:CUI)
explode_dbxrefs_UMLS = \
    explode_dbxrefs[explode_dbxrefs['node_dbxrefs'].str.contains('UMLS:C') == True].groupby('node_id', sort=False, group_keys=False)[
        'nodeXrefCodes'].progress_apply(list).reset_index(name='nodeCUIs')
# Associate nodes with cross-references that have UMLS CUIs.
# In general, this could result in a node having more than one row, if the node was associated with more than one
# UMLS CUI.
node_metadata = node_metadata.merge(explode_dbxrefs_UMLS, how='left', on='node_id')


del explode_dbxrefs_UMLS

del explode_dbxrefs['nodeXrefCodes']

# --------------------------------------------------
# ASSOCIATE NODE CODES WITH UMLS CUIS
# --i.e., if the node is defined in the UMLS CSVs.

# ### Get the UMLS CUIs for each node_id from CUI-CODEs file as CUI_CODEs

# In[13]:

# Read the large CUI-CODES.csv reference.
ulog.print_and_logger_info('-- Reading existing CUIs from CUI-CODES.csv...')
#CUI_CODEs = pd.read_csv(csv_path("CUI-CODEs.csv"))
CUI_CODEs = uextract.read_csv_with_progress_bar(path=csv_path("CUI-CODEs.csv"))
CUI_CODEs = CUI_CODEs.dropna().drop_duplicates().reset_index(drop=True)

# #### A big groupby - ran a couple of minutes - changed groupby to not sort the keys to speed it up

# JAS 5 OCT 2022
# Enhancement to force case insensitivity of checking cross-referenced concepts.

# In general, the code for a concept in an ontology can be *mixed case*. Examples include:
# 1. EDAM: operation_00004
# 2. HUSAT: SampleMedia
# An earlier step in the script converts the codes to uppercase in node_metadata.
# Later merges with node_metadata on node_id requires that the right side also be uppercase.
# Without this conversion, dbxrefs for which the code is mixed case will be ignored, because they still
# exist in mixed case in CUI_CODEs.csv.

# In[14]:

ulog.print_and_logger_info('-- Flattening data from CUI_CODEs...')

CODE_CUIs = CUI_CODEs.groupby(':END_ID', sort=False)[':START_ID'].progress_apply(list).reset_index(name='CUI_CODEs')

# JAS the call to upper is new.
ulog.print_and_logger_info('-- Merging data from CUI_CODEs with node metadata...')
node_metadata = node_metadata.merge(CODE_CUIs, how='left', left_on='node_id', right_on=CODE_CUIs[':END_ID'].str.upper())

del CODE_CUIs
del node_metadata[':END_ID']

# ### Add column for Xref's CUIs - merge exploded_node_metadata with CUI_CODEs then group,
# eliminate duplicates and merge with node_metadata

# In[15]:

# JAS 5 OCT 2022
# In general, the code for a concept in an ontology can be mixed case. Examples include:
# 1. EDAM: operation_00004
# 2. HUSAT (a deprecated SAB): SampleMedia
# An earlier step converts the codes to uppercase in node_metadata, so later merges with node_metadata
# on node_id requires that the right side also be uppercase. Without this conversion, dbxrefs
# for which the code is mixed case will be ignored.

# Original code
# node_xref_cui = explode_dbxrefs.merge(CUI_CODEs, how='inner', left_on='node_dbxrefs', right_on=':END_ID')
# New code uses upper.

ulog.print_and_logger_info('-- Identifying non-UMLS cross-references from CUI-CODEs.csv...')

node_xref_cui = explode_dbxrefs.merge(CUI_CODEs, how='inner', left_on='node_dbxrefs',
                                      right_on=CUI_CODEs[':END_ID'].str.upper())
# JAS FEB 2023 Adding group_keys=False to silence the FutureWarning.
node_xref_cui = node_xref_cui.groupby('node_id', sort=False, group_keys=False)[':START_ID'].progress_apply(list).reset_index(name='XrefCUIs')
node_xref_cui['XrefCUIs'] = node_xref_cui['XrefCUIs'].apply(lambda x: pd.unique(x)).apply(list)
node_metadata = node_metadata.merge(node_xref_cui, how='left', on='node_id')
del node_xref_cui
del explode_dbxrefs

# ### Add column for base64 CUIs

# In[16]:


def base64it(x):

    return [base64.urlsafe_b64encode(str(x).encode('UTF-8')).decode('ascii')]


# The CUI for the node will be the base64-encoded value of the concatenated SAB and code.
# JAS MARCH 2023
# EXPLANATION: the base64-encoded CUI will be assigned to a node if the node is brand new to the
# knowledge graph--i.e., if it cannot be associated by either direct reference or cross-reference to another
# CUI that already exists in the knowledge graph at the time of ingestion.

# July 2023 - CUIs for non-UMLS codes are no longer base64-encoded.
ulog.print_and_logger_info('-- Defining CUIs for nodes...')
node_metadata['base64cui'] = node_metadata['node_id']+' CUI'#.apply(base64it)

# ### Add cuis list and preferred cui to complete the node "atoms" (code, label, syns, xrefs, cuis, CUI)

# In[17]:

ulog.print_and_logger_info('-- Ordering CUIs for nodes by preference (UMLS cross-reference CUI > UMLS direct reference CUI > non-UMLS cross-reference CUI > new CUI) ...')
# create correct length lists
node_metadata['cuis'] = node_metadata['base64cui']
node_metadata['CUI'] = ''

# join list across row
# This arranges CUIs for nodes in order of preference:
# 1. nodeCUIs - the UMLS CUIs for cross-references that existed previously in the ontology CSVs
# 2. CUI_CODEs - the UMLS CUI for the node that existed previously in the ontology CSVs
# 3. XrefCUIs - the non-UMLS CUIs for a cross-reference that existed previously in the ontology CSVs
# 4. base64cui - the new CUI (formerly base64 encoding of the node id) for a node that does not currently
#                exist in the ontology CSVs

# July 2023 - After disabling base64 encoding of CUIs, it is necessary to convert the value of the base64cui field
# to a list; otherwise, the subsequent tolist function treats the field as a character list.

node_metadata['base64cui'] = node_metadata[['base64cui']].values.tolist()
node_metadata['cuis'] = node_metadata[['nodeCUIs', 'CUI_CODEs', 'XrefCUIs', 'base64cui']].values.tolist()

# remove nan, flatten, and remove duplicates - retains order of elements which is key to consistency
node_metadata['cuis'] = node_metadata['cuis'].apply(lambda x: [i for i in x if i == i])
node_metadata['cuis'] = node_metadata['cuis'].apply(lambda x: [i for row in x for i in row])
node_metadata['cuis'] = node_metadata['cuis'].apply(lambda x: pd.unique(x)).apply(list)


# JAS 27 MAR 2023
# (Documentation updated in August 2023)
# Assign the preferred CUI for the node.
# The 'cuis' list for a node is ordered in terms of preference (see above) by type. In addition, we assume
# that if XrefCUIs and CUI_CODEs are lists, the elements in the list are arranged in order of preference--
# e.g., as codes are listed in dbxref. For these reasons, the first element in the cuis list for a node is
# assumed to have the highest preference.

# It is possible for multiple nodes to be associated with the same CUI--e.g., through cross-references. However, this is a
# requirement for ontologies that have higher resolution than the ontologies to which they cross-reference--e.g., Azimuth cell mappings to CL codes.
# It is also possible for two codes to be cross-referenced to non-UMLS codes that map to the same UMLS CUI--e.g.,
# CL codes that cross-reference to FMA codes that are mapped to the same UMLS CUI.

# Because a cross-reference in a nodes file may already have been cross-referenced in a prior ingestion, it
# is possible that the assignment of a cross-referenced CUI will result in self-referential mappings--
# i.e., relationships in format CUI X (relationship) CUI X, where X=X.
# Self-referential mappings will be removed prior to appending to CUI-CUIs.CSV.

# The default assignment is to the first CUI in the list. This works for the majority of codes.
node_metadata['CUI'] = node_metadata['cuis'].str[0]

# August 2023
# Address case in which multiple codes are mapped to the same CUI.
# Logic:
# 1. Find duplicate code-CUI assignments--i.e., CUIs with a group of more than one associated code.
# 2. For each code that is in a group of duplicates, look for the best alternate CUI for the code, selecting from the
#    ordered set of possible CUIs. Start with the default (first).
# This logic is similar to the original assignment logic, with the significant difference that it only
# works with known duplicates instead of looping through the entire node list.

ulog.print_and_logger_info('-- Resolving duplicate code-CUI assignments...')
# Find duplicate code-CUI assignments.
ulog.print_and_logger_info('--- Finding duplicate assignments...')
node_metadata_duplicates = node_metadata.groupby(['CUI']).count().reset_index()
node_metadata_duplicates = node_metadata_duplicates[node_metadata_duplicates['node_id'] > 1]

ulog.print_and_logger_info('--- Assigning alternate CUIs for codes mapped to the same CUI...')
# For each CUI with multiple codes assigned to it:
for cui in tqdm(node_metadata_duplicates['CUI']):
    dfduplicatenodes = node_metadata[node_metadata['CUI'] == cui]
    cui_assigned = []
    # For each code in the group, find the first CUI that has not already been assigned either to the
    # code itself or another code in the group.
    for index, rows in dfduplicatenodes.iterrows():
        assigned = False
        for c in rows['cuis']:
            if not (c in cui_assigned):
                if not assigned:
                    cui_assigned.append(c)
                    assigned = True
        # If no CUI was assigned, map to the new CUI minted for the code. This is to address an edge case first
        # encountered in MP, in which CL:0000792 is both defined in the node file and listed
        # as a dbxref for MP nodes MP:0010169,MP:0008397, and MP:0010168.
        if not assigned:
            # Assign the new CUI. Because this has been converted to a list for building cuilist,
            # convert to a string.
            c = ''.join(rows['base64cui'])
            cui_assigned.append(c)

    # Revise CUI assignments to the codes in the group.
    node_metadata.loc[node_metadata['CUI'] == cui, 'CUI'] = cui_assigned

# -----------------------------------------

# ### Join CUI from node_metadata to each edgelist subject and object


# #### ASSEMBLE CUI-CUIs
ulog.print_and_logger_info('-- Assembling CUI-CUI (concept-concept) relationships...')
# In[18]:
# Merge subject and object nodes with their CUIs; drop the codes; and add the SAB.

# The product will be a DataFrame in the format
# CUI1 relation CUI2 inverse
# e.g.,
# CUI1 isa CUI2 inverse_isa

# JULY 2023 - The algorithm that assigns CUI to nodes based on whether the node is in node_metadata
# or CUI-CODES was generalized for both subject and object nodes in the edgelist.

# Assign CUIs to subject nodes.

# (paleo, as in prior to 2022) original code:
# edgelist = edgelist.merge(node_metadata, how=mergehow, left_on='subject', right_on='node_id')
# JAS 6 JANUARY 2023 - add evidence_class
# edgelist = edgelist[['CUI', 'relation_label', 'object', 'inverse', 'evidence_class']]
# edgelist.columns = ['CUI1', 'relation_label', 'object', 'inverse', 'evidence_class']

# Account for case of nodes in assertions that are external to the set of assertions. The
# use cases include
# 1. the mapping of UNIPROTKB proteins to genes in HGNC
# 2. Metabolics Workbench

# A node (subject or object) in the edgelist can be one of three types:
# 1. A node that is also in node_metadata. This is the *expected* case for subjects
#    and objects that are owned by the SAB that also owns the assertions--e.g., UBERON nodes in the UBERON ontology.
#    The assumption is that the node will be new to the UBKG, and so should be defined in the node file.
# 2. A node that is not in node_metadata, but may be in the CUI-CODEs data.
#    This is the case for assertions that involve nodes from ontologies that were previously ingested--e.g.,
#    UBERON nodes in the PATO ontology.
# 3. A node that is in neither in node_metadata nor CUI-CODEs. Assertions involving this node will not be ingested.

#    If a node is not in node_metadata, it is assumed that the codes and CUIs for object nodes
#    conform to their representation in CUI-CODEs.

# In general, a node column in edgelist would be of a combination of all three types of nodes--i.e., with
# some object nodes  defined in node_metadata and others in CUI-CODEs. (The third type, of course, is
# not desired.) It is necessary to check both possibilities for each node in the edge list.

# Check first for subject nodes in node_metadata--i.e., of type 1.
# Matching CUIs will be in a field named 'CUI', from node_metadata.
subjnode1 = edgelist.merge(node_metadata, how='inner', left_on='subject', right_on='node_id')
subjnode1 = subjnode1[['subject', 'CUI']]
subjnode1 = subjnode1.drop_duplicates(subset=['subject'])

# Check for subject nodes in CUI_CODEs--i.e., of type 2. Matching CUIs will be in a field named ':END_ID'.
subjnode2 = edgelist.merge(CUI_CODEs, how='inner', left_on='subject', right_on=':END_ID')
# Add evidence_class
subjnode2 = subjnode2[['subject', ':START_ID']]
subjnode2 = subjnode2.drop_duplicates(subset=['subject'])

# Concatenate subjNode1 and subjNode2 to allow for conditional selection of CUI.
# The result is a DataFrame with columns for each node:
# subject CUI relation_label inverse CUI :START_ID

#subjnode = subjnode1.merge(subjnode2, how='inner', left_on='subject', right_on='subject')
subjnode = pd.concat([subjnode1, subjnode2]).drop_duplicates()
subjnode = subjnode[['subject', 'CUI', ':START_ID']]

# If CUI is non-null, then the node is of the first type; otherwise, it is likely of the second type.
subjnode['CUI1'] = subjnode[':START_ID'].where(subjnode['CUI'].isna(), subjnode['CUI'])

edgelist = edgelist.merge(subjnode, how='left', left_on='subject', right_on='subject')
edgelist = edgelist[['subject', 'CUI1', 'relation_label', 'object', 'inverse', 'evidence_class']]

# Report on subject nodes that were neither in node_metadata nor CUI-CODEs--i.e., type 3.
subjnode3 = edgelist[edgelist['CUI1'] == '']
ubkg_report.report_missing_node(nodetype='subject', dfmissing=subjnode3)

# July 2023- remove type 3 nodes.
edgelist = edgelist.dropna(subset=['CUI1'])

del subjnode
del subjnode1
del subjnode2
del subjnode3

# -------
# Assign CUIs to object nodes.

# (paleo) original code
# edgelist = edgelist.merge(node_metadata, how='left', left_on='object', right_on='node_id')
# edgelist = edgelist[['CUI1','relation_label','CUI','inverse']]
# edgelist.columns = ['CUI1','relation_label','CUI2','inverse']

# Check first for object nodes in node_metadata--i.e., of type 1.
objnode1 = edgelist.merge(node_metadata, how='inner', left_on='object', right_on='node_id')
objnode1 = objnode1[['object', 'CUI']]
objnode1 = objnode1.drop_duplicates(subset=['object'])

# Check for object nodes in CUI_CODEs--i.e., of type 2. Matching CUIs will be in a field named ':END_ID'.
objnode2 = edgelist.merge(CUI_CODEs, how='inner', left_on='object', right_on=':END_ID')
objnode2 = objnode2[['object', ':START_ID']]
objnode2 = objnode2.drop_duplicates(subset=['object'])

# Concatenate objNode1 and objNode2 to allow for conditional selection of CUI.
# The union will result in a DataFrame with columns for each node:
# object CUI1 relation_label inverse CUI :START_ID

objnode = pd.concat([objnode1, objnode2]).drop_duplicates()


# If CUI is non-null, then the node is of the first type; otherwise, it is likely of the second type.
objnode['CUI2'] = objnode[':START_ID'].where(objnode['CUI'].isna(), objnode['CUI'])

edgelist = edgelist.merge(objnode, how='left', left_on='object', right_on='object')
edgelist = edgelist[['subject', 'CUI1', 'relation_label', 'object', 'CUI2', 'inverse', 'evidence_class']]

# Report on object nodes that were neither in node_metadata nor CUI-CODEs--i.e., type 3.
objnode3 = edgelist[edgelist['CUI2'] == '']
ubkg_report.report_missing_node(nodetype='object', dfmissing=objnode3)

# July 2023 - Remove type 3 object nodes.
edgelist = edgelist.dropna(subset=['CUI2'])

del objnode
del objnode1
del objnode2
del objnode3


# original code
# edgelist = edgelist.dropna().drop_duplicates().reset_index(drop=True)
# edgelist = edgelist[['CUI1','relation_label','CUI','inverse']]

# JAS 6 JANUARY 2023 - Add evidence_class
# edgelist.columns = ['CUI1', 'relation_label', 'CUI2', 'inverse', 'evidence_class']
# JAS 6 JANUARY 2023 - subset to account for optional evidence_class
# subset = ['CUI1', 'relation_label', 'CUI2', 'inverse']
# edgelist = edgelist.dropna(subset=subset).drop_duplicates(subset=subset).reset_index(drop=True)

edgelist = edgelist[['CUI1', 'relation_label', 'CUI2', 'inverse', 'evidence_class']]
edgelist['SAB'] = OWL_SAB

edgelist = edgelist.drop_duplicates(subset=['CUI1', 'relation_label', 'CUI2'])


# -------------------------------------------------
# JAS 27 MAR 2023
# Remove "self-references"--i.e., assertions in which the CUIs for subject and object nodes are identical.
# Self-references are introduced in the assignment of nodes to preferred CUIs, in the case where the CUI is for
# a cross-reference that is itself cross-referenced to another CUI.

ulog.print_and_logger_info('-- Removing self-reference edges introduced from CUI assignment...')
edgelist = edgelist[edgelist['CUI1'] != edgelist['CUI2']]

# --------------------------------------------------
# ## Write out files
ulog.print_and_logger_info('APPENDING TO ONTOLOGY CSVs...')

# ### Test existence when appropriate in original csvs and then add data for each csv

# JAS MARCH 2023 - EXPLANATION
# CUI-CUIs.csv contains information on relationships between concepts.

# #### Write CUI-CUIs (':START_ID', ':END_ID', ':TYPE', 'SAB', 'evidence_class')
# (no prior-existence-check because want them in this SAB)

# In[19]:
ulog.print_and_logger_info('-- Appending to CUI-CUIs.csv...')

# TWO WRITES comment out during development

# JAS 6 JAN 2023 Add evidence_class column to file.
# ulog.print_and_logger_info('---- Adding evidence_class column to CUI-CUIs.csv...')
fcsv = csv_path('CUI-CUIs.csv')
# evidence_class should be a string.
new_header_columns = [':START_ID', ':END_ID', ':TYPE','SAB', 'evidence_class:string']
uextract.update_columns_to_csv_header(file=fcsv, new_columns=new_header_columns, fill=True)

ulog.print_and_logger_info('---- Appending forward relationships...')
# forward ones
# JAS 6 JAN 2023 Add evidence_class. (The SAB column was added after evidence_class above.)
edgelist.columns = [':START_ID', ':TYPE', ':END_ID', 'inverse', 'evidence_class', 'SAB']


if edgelist.shape[0] > TQDM_THRESHOLD:
    uextract.to_csv_with_progress_bar(df=edgelist[[':START_ID', ':END_ID', ':TYPE', 'SAB', 'evidence_class']],
                                      path=csv_path('CUI-CUIs.csv'), mode='a', header=False, index=False)
else:
    edgelist[[':START_ID', ':END_ID', ':TYPE', 'SAB', 'evidence_class']].to_csv(csv_path('CUI-CUIs.csv'), mode='a',
                                                                            header=False, index=False)

ulog.print_and_logger_info('---- Appending inverse relationships...')
# reverse ones
# JAS 6 JAN 2023 Add evidence_class. (The SAB column was added after evidence_class above.)
edgelist.columns = [':END_ID', 'relation_label', ':START_ID', ':TYPE', 'evidence_class', 'SAB']
if edgelist.shape[0] > TQDM_THRESHOLD:
    uextract.to_csv_with_progress_bar(df=edgelist[[':START_ID', ':END_ID', ':TYPE', 'SAB', 'evidence_class']],
                                      path=csv_path('CUI-CUIs.csv'), mode='a', header=False, index=False)
else:
    edgelist[[':START_ID', ':END_ID', ':TYPE', 'SAB', 'evidence_class']].to_csv(csv_path('CUI-CUIs.csv'), mode='a',
                                                                            header=False, index=False)

del edgelist

# #### Write CODEs (CodeID:ID,SAB,CODE,value,lowerbound,upperbound,unit) - with existence check against CUI-CODE.csv

# JAS MARCH 2023 - EXPLANATION
# CODEs.csv contains information on codes in the SABs that are in the knowledge graph.

# In[20]:

ulog.print_and_logger_info('-- Appending to CODEs.csv...')

# JAS 6 JAN 2023 Add value, lowerbound, upperbound, unit column to file.
# ulog.print_and_logger_info('---- Adding value, lowerbound, upperbound, unit columns to CODES.csv...')
fcsv = csv_path('CODEs.csv')
# value, lowerbound, and upperbound should be numbers.
new_header_columns = ['CodeID:ID', 'SAB', 'CODE', 'value:float', 'lowerbound:float', 'upperbound:float', 'unit']
uextract.update_columns_to_csv_header(file=fcsv, new_columns=new_header_columns, fill=True)

# JAS 6 JAN 2023 add value, lowerbound, upperbound, unit
newCODEs = node_metadata[['node_id', 'SAB', 'CODE', 'CUI_CODEs', 'value', 'lowerbound', 'upperbound', 'unit']]

newCODEs = newCODEs[newCODEs['CUI_CODEs'].isnull()]
newCODEs = newCODEs.drop(columns=['CUI_CODEs'])
newCODEs = newCODEs.rename({'node_id': 'CodeID:ID'}, axis=1)

# JAS 6 JAN 2023 add subset to ignore optional columns. Add fillna to remove NaNs from optional columns.
subset = ['CodeID:ID', 'SAB', 'CODE']
newCODEs = newCODEs.dropna(subset=subset).drop_duplicates(subset=subset).reset_index(drop=True).fillna('')
# write/append - comment out during development
ulog.print_and_logger_info('---- Appending...')
if newCODEs.shape[0] > TQDM_THRESHOLD:
    uextract.to_csv_with_progress_bar(df=newCODEs, path=csv_path('CODEs.csv'), mode='a', header=False, index=False)
else:
    newCODEs.to_csv(csv_path('CODEs.csv'), mode='a', header=False, index=False)


del newCODEs

# #### Write CUIs (CUI:ID) - with existence check against CUI-CODE.csv

# JAS MARCH 2023 - EXPLANATION:
# CUIs.CSV contains CUIs for all concepts in the knowledge graph.

# In[21]:

ulog.print_and_logger_info('-- Appending to CUIs.csv...')
CUIs = CUI_CODEs[[':START_ID']].dropna().drop_duplicates().reset_index(drop=True)
CUIs.columns = ['CUI:ID']

newCUIs = node_metadata[['CUI']]
newCUIs.columns = ['CUI:ID']

# Here we isolate only the rows not already matching in existing files
df = newCUIs.drop_duplicates().merge(CUIs.drop_duplicates(), on=CUIs.columns.to_list(), how='left', indicator=True)
newCUIs = df.loc[df._merge == 'left_only', df.columns != '_merge']
newCUIs.reset_index(drop=True, inplace=True)

newCUIs = newCUIs.dropna().drop_duplicates().reset_index(drop=True)
# write/append - comment out during development
if newCUIs.shape[0] > TQDM_THRESHOLD:
    uextract.to_csv_with_progress_bar(df=newCUIs, path=csv_path('CUIs.csv'), mode='a', header=False, index=False)
else:
    newCUIs.to_csv(csv_path('CUIs.csv'), mode='a', header=False, index=False)

# del newCUIs - do not delete here because we need newCUIs list later
# del CUIs


# #### Write CUI-CODEs (:START_ID,:END_ID) - with existence check against CUI-CODE.csv

# JAS March 2023
# EXPLANATION:
# CUI-CODEs.csv links codes (in different SABs) to concepts.

# The purpose of the prior code that assigns a "preferred CUI" for each node is to optimize the association
# of nodes with CUIs that were defined prior to the current ingestion. The "base64" CUI is the CUI of
# last resort, and corresponds to a CUI that was created during this ingestion.
# At this point in the script, each node is associated with one of the following, in order of preference:
# 1. a UMLS CUI that is a direct reference
# 2. a UMLS CUI that is a cross-reference
# 3. a non-UMLS CUI (i.e., from a prior ingestion) that is a cross-reference
# 4. a new base64 CUI

# In[22]:
ulog.print_and_logger_info('-- Appending to CUI_CODES.csv...')

# The last CUI in cuis is always base64 of node_id - here we grab those only if they are the selected CUI (and all CUIs)
newCUI_CODEsCUI = node_metadata[['CUI', 'node_id']]
newCUI_CODEsCUI.columns = [':START_ID', ':END_ID']

# Here we grab all the rest of the cuis except for last in list (excluding single-length cuis lists first)
newCUI_CODEscuis = node_metadata[['cuis', 'node_id']][node_metadata['cuis'].apply(len) > 1]
newCUI_CODEscuis['cuis'] = newCUI_CODEscuis['cuis'].apply(lambda x: x[:-1])
newCUI_CODEscuis = newCUI_CODEscuis.explode('cuis')[['cuis', 'node_id']]
newCUI_CODEscuis.columns = [':START_ID', ':END_ID']

newCUI_CODEs = pd.concat([newCUI_CODEsCUI, newCUI_CODEscuis], axis=0).dropna().drop_duplicates().reset_index(drop=True)

# Here we isolate only the rows not already matching in existing files
df = newCUI_CODEs.merge(CUI_CODEs, on=CUI_CODEs.columns.to_list(), how='left', indicator=True)
newCUI_CODEs = df.loc[df._merge == 'left_only', df.columns != '_merge']
newCUI_CODEs = newCUI_CODEs.dropna().drop_duplicates().reset_index(drop=True)

# write/append - comment out during development
if newCUI_CODEs.shape[0] > TQDM_THRESHOLD:
    uextract.to_csv_with_progress_bar(df=newCUI_CODEs, path=csv_path('CUI-CODEs.csv'), mode='a', header=False,
                                      index=False)

else:
    newCUI_CODEs.to_csv(csv_path('CUI-CODEs.csv'), mode='a', header=False, index=False)


del newCUI_CODEsCUI
del newCUI_CODEscuis
del df
del newCUI_CODEs

# #### Load SUIs from csv

# SEPTEMBER 2023
# The SUIs.csv file has one column, containing a unique set of term strings. The column header is 'name:ID';
# the :ID indicates to the neo4j-admin import that name is the ID field for Term nodes.
# Prior to September 2023, the SUIs.CSV used a SUI:ID and name columns. The SUI property was removed.
#

# JAS March 2023 - explanation: SUIs correspond to term values.

# In[23]:

SUIs = pd.read_csv(csv_path("SUIs.csv"))
# SUIs supposedly unique but...discovered 5 NaN names in SUIs.csv and drop them here
# ?? from ASCII converstion for Oracle to Pandas conversion on original UMLS-Graph-Extracts ??
SUIs = SUIs.dropna().drop_duplicates().reset_index(drop=True)

# SEPT 2023 - SUI:ID removed
# #### Write SUIs (SUI:ID,name) part 1, from label - with existence check

# In[24]:
ulog.print_and_logger_info('-- Appending terms to SUIs.csv...')

# JAS APR/June 2023
# Only look for SUIs if the nodes in the node file have labels.

if node_metadata_has_labels:

    # SEPT 2023 - SUI:ID removed.
    # newSUIs = node_metadata.merge(SUIs, how='left', left_on='node_label', right_on='name')[['node_id', 'node_label', 'CUI', 'SUI:ID', 'name']]
    newSUIs = node_metadata.merge(SUIs, how='left', left_on='node_label', right_on='name:ID')[['node_id', 'node_label', 'CUI', 'name:ID']]

    # June 2023
    # Drop duplicates. This became an issue with the Data Distillery ingests.
    newSUIs = newSUIs.dropna(subset='node_label')
    newSUIs = newSUIs.drop_duplicates(subset='node_label').reset_index(drop=True)

    # SEPT 2023 - SUI:ID removed
    # for Term.name that don't join with node_label update the SUI:ID with base64 of node_label
    # newSUIs.loc[(newSUIs['name'] != newSUIs['node_label']), 'SUI:ID'] = \
        # newSUIs[newSUIs['name'] != newSUIs['node_label']]['node_label'].apply(base64it).str[0]

    # change field names and isolate non-matched ones (don't exist in SUIs file)
    # SEPT 2023 - SUI:ID removed; name now name:ID
    # newSUIs.columns = ['node_id', 'name', 'CUI', 'SUI:ID', 'OLDname']
    newSUIs.columns = ['node_id', 'name:ID', 'CUI', 'OLDname']
    # newSUIs = newSUIs[newSUIs['OLDname'].isnull()][['node_id', 'name', 'CUI', 'SUI:ID']]
    newSUIs = newSUIs[newSUIs['OLDname'].isnull()][['node_id', 'name:ID', 'CUI']]
    newSUIs = newSUIs.dropna().drop_duplicates().reset_index(drop=True)
    # newSUIs = newSUIs[['SUI:ID', 'name']]
    newSUIs = newSUIs[['name:ID']]

    # update the SUIs dataframe to total those that will be in SUIs.csv
    SUIs = pd.concat([SUIs, newSUIs], axis=0).reset_index(drop=True)

    # write out newSUIs - comment out during development
    if newSUIs.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(df=newSUIs, path=csv_path('SUIs.csv'), mode='a', header=False, index=False)
    else:
        newSUIs.to_csv(csv_path('SUIs.csv'), mode='a', header=False, index=False)


# del newSUIs - not here because we use this dataframe name later

# #### Write CUI-SUIs (:START_ID,:END_ID)
# JAS MARCH 2023
# EXPLANATION: CUI-SUIs links terms for concepts--i.e., Terms with relationship PREF_TERM.

# SEPT 2023 CUI-SUIs is used in the neo4j-admin import to establish terms of type PREF_TERM for Concept nodes--
# i.e., Term nodes that have a relationship of type PREF_TERM. The neo4j-admin import expects two fields named
# :START_ID and :END_ID in CSVs used to import relationships. In CUI-SUIs, :START_ID is the CUI of the Concept
# node and :END_ID is the name (ID) of the Term node that will have a rleationship of type PREF_TERM.
# Prior to September 2023, :END_ID corresponded to the SUI for the Term node.

ulog.print_and_logger_info('-- Appending terms to CUI-SUIs.csv...')
# In[25]:

# get the newCUIs associated metadata (CUIs are unique in node_metadata)

newCUI_SUIs = newCUIs.merge(node_metadata, how='inner', left_on='CUI:ID', right_on='CUI')

# JAS APR 2023 Only look for SUIs if there are values for node_label.
if node_metadata_has_labels:
    newCUI_SUIs = newCUI_SUIs[['node_label', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)

    # get the SUIs matches

    # SEPT 2023 - SUI:ID replaced with name.
    # newCUI_SUIs = newCUI_SUIs.merge(SUIs, how='left', left_on='node_label', right_on='name')[
    # ['CUI', 'SUI:ID']].dropna().drop_duplicates().reset_index(drop=True)
    newCUI_SUIs = newCUI_SUIs.merge(SUIs, how='left', left_on='node_label', right_on='name:ID')[
        ['CUI', 'name:ID']].dropna().drop_duplicates().reset_index(drop=True)

    # SEPT 2023 - Drop empty terms.
    newCUI_SUIs = newCUI_SUIs.replace({'': np.nan})
    newCUI_SUIs = newCUI_SUIs.dropna(subset=['name:ID'])
    newCUI_SUIs.reset_index(drop=True, inplace=True)

    newCUI_SUIs.columns = [':START:ID', ':END_ID']

    # write/append - comment out during development
    if newCUI_SUIs.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(df=newCUI_SUIs, path=csv_path('CUI-SUIs.csv'), mode='a', header=False,
                                          index=False)
    else:
        newCUI_SUIs.to_csv(csv_path('CUI-SUIs.csv'), mode='a', header=False, index=False)

# del newCUIs
# del newCUI_SUIs


# #### Load CODE-SUIs and reduce to PT or SY
# JAS MARCH 2023
# EXPLANATION: CODE-SUIs.csv links terms for codes--i.e., terms with relationships that are either PT or SY.

# SEPTEMBER 2023
# The CODE-SUIs.csv file is used by the neo4j-admin import to establish relationships between Code nodes and
# Term nodes. A number of relationship types are possible--e.g., PT, SY.
# The import function expects a :START_ID and an :END_ID column for CSVs that are used to define relationships.
# For CODE-SUIs.csv, :START_ID corresponds to the CodeID property of the Code and :END_ID corresponds to the
# name:ID property of the Term.
# Prior to September 2023, :END_ID corresponded to the SUI:ID of the term.

# In[26]:

ulog.print_and_logger_info('-- Appending terms to CODE-SUIs.csv...')

CODE_SUIs = pd.read_csv(csv_path("CODE-SUIs.csv"))
# AUGUST 2023 - Include ACR for HGNC codes.
# SEPT 2023 - ACR processing is no longer done.
# CODE_SUIs = CODE_SUIs[((CODE_SUIs[':TYPE'] == 'PT') | (CODE_SUIs[':TYPE'] == 'SY') | (CODE_SUIs[':TYPE'] == 'ACR'))]
CODE_SUIs = CODE_SUIs[((CODE_SUIs[':TYPE'] == 'PT') | (CODE_SUIs[':TYPE'] == 'SY'))]
CODE_SUIs = CODE_SUIs.dropna().drop_duplicates().reset_index(drop=True)

# #### Write CODE-SUIs (:END_ID,:START_ID,:TYPE,CUI) part 1, from label - with existence check
# This establishes term types of type PT.

def getnewsuisfortermtype(termtype: str, owlsab: str, dfnode: pd.DataFrame, dfsuis: pd.DataFrame, dfcodesuis: pd.DataFrame) -> pd.DataFrame:

    # September 2023
    # Identifies new terms of a specified type for codes ingested for a SAB.

    # Because an ingested ontology can refer to nodes from other ontologies, it is possible that the
    # ingested ontology defines a term of the specified term type that is different from that defined by the term's ontology.
    # We have defined the business rule that only a node's "owning" ontology should define terms of the specified type--
    # e.g., that only the UBERON SAB should define the PT term for a UBERON code, and only HGNC should define ACR terms.
    #
    # The script distinguish between specified terms from an "owning ontology" from
    # those from an ingesting ontology. Terms from the ingesting ontology will have a "SAB" suffix by default.

    # The specified term types of interest in this script are:
    # PT (preferred term) - for all ontologies except for HGNC
    # ACR (acronym) - for HGNC. HGNC uses terms of type PT for the approved name and ACR for the approved symbol;
    #       however, ingested ontologies that refer to HGNC will usually use the approved symbol for the node_label.

    # CORRECTION:
    # Some SABs, including UBERON, correctly use node_label for HGNC approved names, which
    # results in this script erroneously assigning ACR_SAB labels. Because it is not possible
    # to distinguish definitively between correct and incorrect use of the PT, we will choose the lesser
    # evil of extraneous PT_SABs instead of erroneous ACR_SABs.

    # In the most common use case, the term of specified type for a node is the same in both the owning and
    # ingesting ontology. To eliminate unncessary duplication, the script only adds a "_SAB" term
    # if either it differs from that of the owning ontology or no corresponding term is in the UBKG.

    # For example, suppose the following:
    # 1. The following order of ingestion occurs: PATO, MP, UBERON
    # 2. The MP ontology uses codes from PATO, UBERON, and MP.

    # 1. MP codes from the MP ingestion will have a PT term.
    # 2. PATO codes can have terms with the following relationships:
    #    a. PT - from the ingestion of PATO
    #    b. PT_MB - from the ingestion of MP, if the PT for the code from the PATO ingestion is different from
    #              the PT for the code from the MP ingestion, or if the PATO ingestion refers to a MP code
    #              that was not ingested by MP.
    # 3. UBERON codes can have terms with the following relationships:
    #    a. PT - from the ingestion of UBERON
    #    b. PT_PATO - from the ingestion of PATO, subject to the conditions shown for PATO, case b
    #    c. PT_MP - from the ingestion of MP, subject to PATO, case b

    # Notes:
    # 1. Until September 2023, Term nodes had both a name (term string) and a SUI (semantic unit identifier),
    #    to replicate UMLS architecture. The SUI has been removed from the CSVs; however, notation maintains SUI--i.e.,
    #    SUI is essentially equivalent to term string.
    # 2. It is not possible to eliminate completely the issue of a term having both a termtype_SAB and termtype
    #    relationship with a code during ingestion, because of interdependent ontologies--i.e., two
    #    ontologies that make assertions using codes from each other. The order of ingestion of ontologies governs
    #    the result: for example, if we ingest CL and then UBERON, UBERON codes ingested with CL will have terms
    #    of types PT and PT_CL; if, instead, we ingest UBERON before CL, CL codes ingested with UBERON with have
    #    terms of types PT and PT_UBERON. Appropriate ordering of ingestions may reduce the number of unnecessary
    #    termtype_SAB terms.
    # 3. If a SAB refers to codes from another ontology that is not ingested at all, the codes for that ontology will
    #    only have PT_SAB for that code.
    # 4. Some SABs (e.g., NCBI) do not use terms of type PT, and so will always have PT_SAB terms.
    # 5. Some ontologies from UMLS (e.g., OMIM) also use terms of type ACR; however, these terms will not be
    #    tested with this logic.
    # ---------

    ulog.print_and_logger_info(f'---- Checking for new or differing terms of type {termtype}...')

    # Find all new terms by merging the new codes against the terms list. The terms list was
    # populated with net new terms prior to this function.
    # SEPT 2023 - SUI:ID replaced with name
    # dfnewcodesuis = dfnode.merge(dfsuis, how='left', left_on='node_label', right_on='name')[
       # ['SUI:ID', 'node_id', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)
    dfnewcodesuis = dfnode.merge(dfsuis, how='left', left_on='node_label', right_on='name:ID')[
        ['name:ID', 'node_id', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)

    # Apply the default filter based on specified type.
    # If the SAB for the code is not the same as the ingesting ontology, append the SAB to the term type.
    if termtype == 'PT':
        dfnewcodesuis[':TYPE'] = np.where((owlsab == dfnewcodesuis['node_id'].str.upper().str.split(':').str[0]),
                                     'PT', 'PT_' + owlsab)
        # HGNC uses PT for approved name, not approved symbol. Drop HGNC PTs.

        # SEPT 4 2023 - Filtering dropped. SEE 'CORRECTION' COMMENT ABOVE.
        # dfnewcodesuis = dfnewcodesuis[dfnewcodesuis['node_id'].str.upper().str.split(':').str[0] != 'HGNC']
    # elif termtype == 'ACR':
        # SEPT 4 2023 - SEE 'CORRECTION' COMMENT ABOVE
        # Filter to HGNC ACRs.
        #dfnewcodesuis[':TYPE'] = np.where((owlsab == dfnewcodesuis['node_id'].str.upper().str.split(':').str[0]),
                                          # 'ACR', 'ACR_' + owlsab)
        #vdfnewcodesuis = dfnewcodesuis[dfnewcodesuis['node_id'].str.upper().str.split(':').str[0] == 'HGNC']
    else:
        ulog.print_and_logger_info(f'Invalid type for getnewsuisfortermtype: {termtype}')
        exit(1)


    # SEPT 2023 - SUI:ID replaced with name
    # dfnewcodesuis.columns = [':END_ID', ':START_ID', 'CUI', ':TYPE']
    # neo4j-admin import looks for columns named :START_ID and :END_ID for relationship imports.
    dfnewcodesuis.columns = [':END_ID', ':START_ID', 'CUI',':TYPE']

    # Filter to only the rows not already matching in existing files--
    # i.e., those for which all columns have identical values.
    dfnew = dfnewcodesuis.drop_duplicates().merge(dfcodesuis.drop_duplicates(), on=dfcodesuis.columns.to_list(), how='left',
                                              indicator=True)
    dfnewcodesuis = dfnew.loc[dfnew._merge == 'left_only', dfnew.columns != '_merge']
    dfnewcodesuis.reset_index(drop=True, inplace=True)

    # Logic:
    # Only add a termtype_SAB term if it differs from the termtype term, or if there is no termtype term already.
    # For example, if SAB Y refers to a code from SAB X, add a PT_Y if it differs from PT or there is no PT.

    # 1. Find all existing SUIs of the specified term type and obtain the term string.
    # SEPT 2023 - SUI:ID replaced with name; merge with SUIS.csv no longer necessary.
    dftermtype = dfcodesuis[(dfcodesuis[':TYPE'] == termtype)].drop_duplicates()
    # dftermtype = dftermtype.merge(dfsuis, how='inner', left_on=':END_ID', right_on='SUI:ID')

    # 2. Left merge new SUIs with existing SUIs of the specified term type.
    # Obtain term string for the new terms.
    # SEPT 2023 - SUI:ID replaced with name; merge with SUIS.csv no longer necessary.
    # dfnewcodesuis = dfnewcodesuis.merge(dfsuis, how='inner', left_on=':END_ID', right_on='SUI:ID')
    # dfnewcodesuis = dfnewcodesuis.merge(dfsuis, how='inner', left_on=':END_ID', right_on='name')

    dfnewcodesuis = dfnewcodesuis.merge(dftermtype, how='left', on=[':END_ID',':START_ID'])

    # At this point, the dfnewcodesuis DataFrame has columns
    # name	:START_ID	CUI_x	:TYPE_x	:TYPE_y	CUI_y
    # where _x corresponds to potential new terms and _y corresponds to existing terms.

    # 3. Only keep a new term if one of the following is true:
    #   a. The type (with field name :TYPE_x after the merge) is the specified term type.
    #   b. There is no corresponding existing term (:TYPE_y is null)
    dfnewcodesuis = dfnewcodesuis.loc[(dfnewcodesuis[':TYPE_x'] == termtype) | pd.isnull(dfnewcodesuis[':TYPE_y'])]

    # Restore column headers.
    # SEPT 2023 :END_ID (SUI) replaced with name
    dfnewcodesuis = dfnewcodesuis[[':END_ID',':START_ID',':TYPE_x','CUI_x']]
    # neo4j-admin import looks for columns named :START_ID and :END_ID for relationship imports.
    dfnewcodesuis.columns = [':END_ID', ':START_ID', ':TYPE','CUI']

    return dfnewcodesuis

# JAS APR 2023:
# Only check for SUIs if there is a value for node_label.

if node_metadata_has_labels:
    # Obtain code terms for new codes for which existing terms of type PT do not exist.
    newCODE_SUIs = getnewsuisfortermtype(termtype='PT', owlsab=OWL_SAB, dfnode=node_metadata, dfsuis=SUIs,
                                         dfcodesuis=CODE_SUIs)

    # Some SABs, including UBERON, correctly use node_label for HGNC approved names, which
    # results in this script erroneously assigning ACR_SAB labels. Because it is not possible
    # to distinguish definitively between correct and incorrect use of the PT, we will choose the lesser
    # evil of extraneous PT_SABs instead of erroneous ACR_SABs.

    # Obtain code terms for new HGNC codes for which existing terms of type ACR do not exist.
    # newCODE_SUIsACR = getnewsuisfortermtype(termtype='ACR', owlsab=OWL_SAB, dfnode=node_metadata, dfsuis=SUIs,
                                         # dfcodesuis=CODE_SUIs)
    # Concatenate PT and ACR results.
    # newCODE_SUIs = pd.concat([newCODE_SUIs, newCODE_SUIsACR])

    # write out newCODE_SUIs
    if newCODE_SUIs.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(df=newCODE_SUIs, path=csv_path('CODE-SUIs.csv'), mode='a', header=False, index=False)
    else:
        newCODE_SUIs.to_csv(csv_path('CODE-SUIs.csv'), mode='a', header=False, index=False)


# #### Write SUIs (SUI:ID,name) part 2, from synonyms - with existence check

# Explode and merge the synonyms, which are in a pipe-delimited list in node_synonyms.
ulog.print_and_logger_info('-- Appending synonyms to SUIs.csv...')
explode_syns = node_metadata.explode('node_synonyms')[
    ['node_id', 'node_synonyms', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)

# SEPT 2023 - Drop empty synonyms. These can occur for cases in which the string in node_synonyms
# is an explicit "None", which is a reserved type in Python that Pandas translates to an empty string.

explode_syns = explode_syns.replace({'': np.nan})
explode_syns = explode_syns.dropna(subset=['node_synonyms'])
explode_syns.reset_index(drop=True, inplace=True)

# JAS APR/June 2023 only check SUIs if there are values for node_synonyms.
# SEPT 2023 - check exploded synonyms from which empty values have been excluded.
node_metadata_has_synonyms = len(explode_syns['node_synonyms'].value_counts()) > 0

if node_metadata_has_synonyms:
    # SEPT 2023 - SUI:ID removed.
    # newSUIs = explode_syns.merge(SUIs, how='left', left_on='node_synonyms', right_on='name')[
    # ['node_id', 'node_synonyms', 'CUI', 'SUI:ID', 'name']]
    newSUIs = explode_syns.merge(SUIs, how='left', left_on='node_synonyms', right_on='name:ID')[
        ['node_id', 'node_synonyms', 'CUI', 'name:ID']]

    # for Term.name that don't join with node_synonyms update the SUI:ID with base64 of node_synonyms
    # SEPT 2023 - SUI:ID removed
    # newSUIs.loc[(newSUIs['name'] != newSUIs['node_synonyms']), 'SUI:ID'] = \
    # newSUIs[newSUIs['name'] != newSUIs['node_synonyms']]['node_synonyms'].apply(base64it).str[0]

    # change field names and isolate non-matched ones (don't exist in SUIs file)
    # SEPT 2023 - SUI:ID removed.
    # newSUIs.columns = ['node_id', 'name', 'CUI', 'SUI:ID', 'OLDname']
    # newSUIs = newSUIs[newSUIs['OLDname'].isnull()][['node_id', 'name', 'CUI', 'SUI:ID']]
    newSUIs.columns = ['node_id', 'name:ID', 'CUI', 'OLDname']
    newSUIs = newSUIs[newSUIs['OLDname'].isnull()][['node_id', 'name:ID', 'CUI']]
    newSUIs = newSUIs.dropna().drop_duplicates().reset_index(drop=True)
    # newSUIs = newSUIs[['SUI:ID', 'name']]
    newSUIs = newSUIs[['name:ID']]

    # update the SUIs dataframe to total those that will be in SUIs.csv
    SUIs = pd.concat([SUIs, newSUIs], axis=0).reset_index(drop=True)

    # write out newSUIs - comment out during development
    if newSUIs.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(df=newSUIs, path=csv_path('SUIs.csv'), mode='a', header=False, index=False)
    else:
        newSUIs.to_csv(csv_path('SUIs.csv'), mode='a', header=False, index=False)

    del newSUIs

# #### Write CODE-SUIs (:END_ID,:START_ID,:TYPE,CUI) part 2, from synonyms - with existence check

# In[29]:
ulog.print_and_logger_info('-- Appending synonyms to CODE-SUIs.csv...')
# get the SUIs matches

# JAS APR/June 2023 only merge if node_synonyms has values.
if node_metadata_has_synonyms:
    # SEPT 2023 - replace SUI:ID with name
    # newCODE_SUIs = explode_syns.merge(SUIs, how='left', left_on='node_synonyms', right_on='name')[
    # ['SUI:ID', 'node_id', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)

    newCODE_SUIs = explode_syns.merge(SUIs, how='left', left_on='node_synonyms', right_on='name:ID')[
        ['name:ID', 'node_id', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)

    newCODE_SUIs.insert(2, ':TYPE', 'SY')
    # neo4j-admin import looks for columns named :START_ID and :END_ID.
    newCODE_SUIs.columns = [':END_ID', ':START_ID', ':TYPE', 'CUI']

    # Compare the new and old retaining only new--i.e., drop rows that have the exact same values in both new and existing
    # frames.
    df = newCODE_SUIs.drop_duplicates().merge(CODE_SUIs.drop_duplicates(), on=CODE_SUIs.columns.to_list(), how='left',
                                          indicator=True)
    newCODE_SUIs = df.loc[df._merge == 'left_only', df.columns != '_merge']
    # SEPT 2023 - Drop empty synonyms.
    newCODE_SUIs = newCODE_SUIs.replace({'': np.nan})
    newCODE_SUIs = newCODE_SUIs.dropna(subset=[':END_ID'])
    newCODE_SUIs.reset_index(drop=True, inplace=True)

    # write out newCODE_SUIs - comment out during development
    if newCODE_SUIs.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(df=newCODE_SUIs, path=csv_path('CODE-SUIs.csv'), mode='a', header=False, index=False)
    else:
        newCODE_SUIs.to_csv(csv_path('CODE-SUIs.csv'), mode='a', header=False, index=False)

    del newCODE_SUIs

# #### Write DEFs (ATUI:ID, SAB, DEF) and DEFrel (:END_ID, :START_ID) - with check for any DEFs and existence check
# JAS MARCH 2023
# EXPLANATION: DEF.csv and DEFrel.csv contain information on Definition nodes in the knowledge graph.

# In[30]:
ulog.print_and_logger_info('--Appending to DEFs.csv and DEFrel.csv...')

# SEPT 2023 - Drop empty definitions.
node_metadata['node_definition'] = node_metadata['node_definition'].replace({'': np.nan})
node_metadata = node_metadata.dropna(subset=['node_definition'])
node_metadata.reset_index(drop=True, inplace=True)

# June 2023 - Only process if there are any definitions.
node_metadata_has_definitions = len(node_metadata['node_definition'].value_counts()) > 0

if node_metadata_has_definitions:
    DEFs = pd.read_csv(csv_path("DEFs.csv"))
    DEFrel = pd.read_csv(csv_path("DEFrel.csv")).rename(columns={':START_ID': 'CUI', ':END_ID': 'ATUI:ID'})
    DEF_REL = DEFs.merge(DEFrel, how='inner', on='ATUI:ID')[
        ['SAB', 'DEF', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)
    newDEF_REL = node_metadata[['SAB', 'node_definition', 'CUI']].rename(columns={'node_definition': 'DEF'})

    # Compare the new and old retaining only new
    df = newDEF_REL.drop_duplicates().merge(DEF_REL.drop_duplicates(), on=DEF_REL.columns.to_list(), how='left',
                                            indicator=True)
    newDEF_REL = df.loc[df._merge == 'left_only', df.columns != '_merge']
    newDEF_REL.reset_index(drop=True, inplace=True)

    # Add identifier
    newDEF_REL['ATUI:ID'] = newDEF_REL['SAB'] + " " + newDEF_REL['DEF'] + " " + newDEF_REL['CUI']
    newDEF_REL['ATUI:ID'] = newDEF_REL['ATUI:ID'].apply(base64it).str[0]
    newDEF_REL = newDEF_REL[['ATUI:ID', 'SAB', 'DEF', 'CUI']].dropna().drop_duplicates().reset_index(drop=True)

    # Write newDEFs
    if newDEF_REL.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(df=newDEF_REL[['ATUI:ID', 'SAB', 'DEF']], path=csv_path('DEFs.csv'), mode='a',
                                          header=False, index=False)
    else:
        newDEF_REL[['ATUI:ID', 'SAB', 'DEF']].to_csv(csv_path('DEFs.csv'), mode='a', header=False, index=False)

    # Write newDEFrel
    if newDEF_REL.shape[0] > TQDM_THRESHOLD:
        uextract.to_csv_with_progress_bar(
            df=newDEF_REL[['ATUI:ID', 'CUI']].rename(columns={'ATUI:ID': ':END_ID', 'CUI': ':START_ID'}),
            path=csv_path('DEFrel.csv'), mode='a', header=False, index=False)
    else:
        newDEF_REL[['ATUI:ID', 'CUI']].rename(columns={'ATUI:ID': ':END_ID', 'CUI': ':START_ID'}).to_csv(csv_path('DEFrel.csv'), mode='a', header=False, index=False)

    del DEFs
    del DEFrel
    del DEF_REL
    del newDEF_REL


# JAS Sept 2023
# Remove duplicate rows from all CSVs.

uclean.remove_duplicates(csvpath=csv_path('CODE-SUIs.csv'))
uclean.remove_duplicates(csvpath=csv_path('CODEs.csv'))
uclean.remove_duplicates(csvpath=csv_path('CUI-CODEs.csv'))
uclean.remove_duplicates(csvpath=csv_path('CUI-CUIs.csv'))
uclean.remove_duplicates(csvpath=csv_path('CUI-SUIs.csv'))
uclean.remove_duplicates(csvpath=csv_path('CUI-TUIs.csv'))
uclean.remove_duplicates(csvpath=csv_path('CUIs.csv'))
uclean.remove_duplicates(csvpath=csv_path('DEFrel.csv'))
uclean.remove_duplicates(csvpath=csv_path('DEFs.csv'))
uclean.remove_duplicates(csvpath=csv_path('SUIs.csv'))
uclean.remove_duplicates(csvpath=csv_path('TUIrel.csv'))
uclean.remove_duplicates(csvpath=csv_path('TUIs.csv'))

# --------------------------------------------------
# QC reporting, Workflow point 3
# Statistics on final ontology CSVs
# Comparisons of dbxrefs with CODEs.csv.
ulog.print_and_logger_info('QC reporting: ontology CSV statistics')
CODEs = uextract.read_csv_with_progress_bar(path=csv_path("CODEs.csv"))
ubkg_report.CODEs = CODEs
ubkg_report.report_ontology_csv_statistics()
